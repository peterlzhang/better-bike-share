{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## output bikeshare density for what area could support\n",
    "## output bikeability based on strava data to decide if area is a good idea\n",
    "## Assumes that bike shares are optimally placed and near optimal capacity\n",
    "## use regularization in the training of the ML model\n",
    "\n",
    "import pandas as pd\n",
    "import pandas_profiling\n",
    "import geopandas as gpd\n",
    "import osmnx as ox\n",
    "import folium\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from shapely.geometry import Point, Polygon, MultiPolygon\n",
    "from shapely.ops import nearest_points\n",
    "import branca.colormap as cm\n",
    "from pprint import pprint\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import joblib\n",
    "\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Workaround to fix chrome issue where folium won't plot maps with a large number of layers\n",
    "# See comment by dstein64 at: https://github.com/python-visualization/folium/issues/812\n",
    "\n",
    "import base64\n",
    "def _repr_html_(self, **kwargs):\n",
    "    html = base64.b64encode(self.render(**kwargs).encode('utf8')).decode('utf8')\n",
    "    onload = (\n",
    "        'this.contentDocument.open();'\n",
    "        'this.contentDocument.write(atob(this.getAttribute(\\'data-html\\')));'\n",
    "        'this.contentDocument.close();'\n",
    "    )\n",
    "    if self.height is None:\n",
    "        iframe = (\n",
    "            '<div style=\"width:{width};\">'\n",
    "            '<div style=\"position:relative;width:100%;height:0;padding-bottom:{ratio};\">'\n",
    "            '<iframe src=\"about:blank\" style=\"position:absolute;width:100%;height:100%;left:0;top:0;'\n",
    "            'border:none !important;\" '\n",
    "            'data-html={html} onload=\"{onload}\" '\n",
    "            'allowfullscreen webkitallowfullscreen mozallowfullscreen>'\n",
    "            '</iframe>'\n",
    "            '</div></div>').format\n",
    "        iframe = iframe(html=html, onload=onload, width=self.width, ratio=self.ratio)\n",
    "    else:\n",
    "        iframe = ('<iframe src=\"about:blank\" width=\"{width}\" height=\"{height}\"'\n",
    "                  'style=\"border:none !important;\" '\n",
    "                  'data-html={html} onload=\"{onload}\" '\n",
    "                  '\"allowfullscreen\" \"webkitallowfullscreen\" \"mozallowfullscreen\">'\n",
    "                  '</iframe>').format\n",
    "        iframe = iframe(html=html, onload=onload, width=self.width, height=self.height)\n",
    "    return iframe\n",
    "\n",
    "folium.branca.element.Figure._repr_html_ = _repr_html_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gridify_polygon(poly,grid_spacing):\n",
    "    # creates a cartesian grid inside polygon with the input grid_spacing\n",
    "    # poly: polygon which we want a grid inside\n",
    "    # grid_spacing: spaceing in lattitude/longitude degrees\n",
    "    poly_xmin,poly_ymin,poly_xmax,poly_ymax = poly.geometry.total_bounds\n",
    "\n",
    "    cols = list(np.arange(poly_xmin,poly_xmax+grid_spacing,grid_spacing))\n",
    "    rows = list(np.arange(poly_ymin,poly_ymax+grid_spacing,grid_spacing))\n",
    "    rows.reverse()\n",
    "\n",
    "    polygons = []\n",
    "    for x in cols:\n",
    "        for y in rows:\n",
    "            polygons.append( Polygon([(x,y), (x+grid_spacing, y), (x+grid_spacing, y-grid_spacing), (x, y-grid_spacing)]) )\n",
    "\n",
    "    grid = gpd.GeoDataFrame({'geometry':polygons})\n",
    "\n",
    "    grid['isin_poly'] = grid.apply(lambda row: row['geometry'].centroid.within(poly.geometry[0]), axis=1)\n",
    "    poly_grid = grid[grid.isin_poly == True]\n",
    "    poly_grid.crs = {'init': 'epsg:4326', 'no_defs': True}\n",
    "    poly_grid = poly_grid.drop(['isin_poly'], axis = 1)\n",
    "    \n",
    "    # Calculate the polygon areas in km\n",
    "    poly_grid_cart = poly_grid.copy()\n",
    "    poly_grid_cart = poly_grid_cart.to_crs({'init': 'epsg:3857'})\n",
    "    poly_grid_cart['poly_area_km'] = poly_grid_cart['geometry'].area/ 10**6\n",
    "    # Store polygon area\n",
    "    poly_grid['poly_area_km'] = poly_grid_cart['poly_area_km']\n",
    "    \n",
    "    # \n",
    "    poly_grid = poly_grid.reset_index()\n",
    "    return poly_grid\n",
    "\n",
    "def amenity_in_polygon(amenity_points,poly):\n",
    "    # returns the amenities that are inside the given polygon\n",
    "    # When there are zero amenities within the interrogation region, the function returns an empty dataframe as\n",
    "    # as expected, but also prints out a lot of errors. not a huge issue but annoying.\n",
    "    # Maybe implement a test for if empty, return 0\n",
    "    # Example use:\n",
    "    #         amenity_in_polygon(food_amenities,city_grid.geometry.iloc[38])\n",
    "    \n",
    "    # Generate boolean list of whether amenity is in polygon\n",
    "    indices = amenity_points.apply(lambda row: row['geometry'].within(poly), axis=1)\n",
    "    if not any(indices): # If all indices are false\n",
    "        return pd.DataFrame(columns=['A']) # return empty dataframe (not sure what is best to output here )\n",
    "    else:\n",
    "        return amenity_points[amenity_points.apply(lambda row: row['geometry'].within(poly), axis=1)]\n",
    "\n",
    "def avg_dist_to_amenities(interrogation_point,amenity_df,n):\n",
    "    # calculates the mean distance of the n nearest amenities to the interrogation point\n",
    "    # If there are less than n amenities in the search it'll just return the average of the known amenities.\n",
    "    # Example: avg_dist_to_amenities(city_grid.geometry.iloc[39],food_amenities,5)\n",
    "    dist_to_amenity = amenity_df['geometry'].apply(lambda x: x.distance(interrogation_point))\n",
    "    dist_to_amenity.sort_values(inplace=True)\n",
    "    dist_to_amenity[:5]\n",
    "    if len(dist_to_amenity) >= n:\n",
    "        return dist_to_amenity[:n].mean()\n",
    "    elif len(dist_to_amenity) == 0:\n",
    "        return np.nan\n",
    "    else:\n",
    "        return dist_to_amenity.mean()\n",
    "    \n",
    "    \n",
    "# Generate features dataframe by finding the count of each unique amenity in each region\n",
    "def features_density(interrogation_grid,osm_features,targets):\n",
    "    # Calculate feature and target density inside a series of polygons\n",
    "    # INPUTS\n",
    "    # ------\n",
    "    # Interrogation grid: list of polygons in which to calculate density of features and targets\n",
    "    # osm_features: gdf of amenities in area retrieved from OSM\n",
    "    # targest: gdf of target locations\n",
    "    # OUTPUTS\n",
    "    # -------\n",
    "    # cleaned_df: contains the density of features and targets in the interrogation grid\n",
    "    # create new cleaned df that will store features and target data\n",
    "    cleaned_df = interrogation_grid.copy()\n",
    "    cleaned_df = cleaned_df.reset_index()\n",
    "    cleaned_df['bike_rental_density'] = 0\n",
    "    cleaned_df = cleaned_df.reindex(cleaned_df.columns.tolist() + amenity_names, axis=1) \n",
    "\n",
    "\n",
    "    # loop through grid points and populate features.\n",
    "    for index, row in cleaned_df.iterrows():\n",
    "        grid_pt = cleaned_df.geometry.iloc[index]\n",
    "        amenities_in_grid = amenity_in_polygon(osm_features,grid_pt)\n",
    "\n",
    "        # fill amenity rows with counts inside each polygon\n",
    "        if len(amenities_in_grid) > 0:\n",
    "            amenity_counts = amenities_in_grid['amenity'].value_counts()\n",
    "            for val, cnt in amenity_counts.iteritems():\n",
    "                # test if value is in list of features that are selected for ML model\n",
    "                if val in amenity_names:\n",
    "                    cleaned_df[val].iloc[index] = cnt / cleaned_df.poly_area_km.iloc[index]\n",
    "\n",
    "        # add target column for bike rentals\n",
    "        bike_rentals_in_grid = amenity_in_polygon(targets,grid_pt)\n",
    "        if len(bike_rentals_in_grid) > 0:\n",
    "            cleaned_df['bike_rental_density'].iloc[index] = len(bike_rentals_in_grid) / cleaned_df.poly_area_km.iloc[index]\n",
    "        else:\n",
    "            cleaned_df['bike_rental_density'].iloc[index] = 0\n",
    "\n",
    "    # remove nan values\n",
    "    cleaned_df[amenity_names] = cleaned_df[amenity_names].fillna(0)\n",
    "    # remove unecessary columns\n",
    "    cleaned_df = cleaned_df.drop(columns = ['level_0','index'])\n",
    "    # relable as density \n",
    "    new_names = [name + '_density' for name in amenity_names]\n",
    "    cleaned_df.rename(columns = dict(zip(amenity_names, new_names)), inplace=True)\n",
    "    \n",
    "    return cleaned_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'amenity_names' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-5b04f207c105>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mamenity_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'amenity_names' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_name = 'OSM_geo_data/'\n",
    "city_names = ['austin','berkeley','boston','chicago','denver','glasgow','los_angeles',\n",
    "              'minneapolis','montreal','new_york','san_francisco','salt_lake_city','phoenix',\n",
    "             'columbus','paris','munich','brussels']\n",
    "\n",
    "\n",
    "# Initialize dataframes\n",
    "df = pd.DataFrame()\n",
    "city_grid = pd.DataFrame()\n",
    "\n",
    "# Combine data from different cities\n",
    "for name in city_names:\n",
    "    filename_cleaned_df = folder_name + name + '_feature_target_table.geojson'\n",
    "    filename_grid = folder_name + name + '_grid.geojson'\n",
    "    if df.empty:\n",
    "        df = gpd.read_file(filename_cleaned_df)\n",
    "        city_grid = gpd.read_file(filename_grid)\n",
    "    else:\n",
    "        df = df.append(gpd.read_file(filename_cleaned_df), ignore_index=True)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature and target labels\n",
    "features = ['pharmacy_density', 'recycling_density', 'place_of_worship_density', 'post_box_density',\n",
    "            'library_density', 'post_office_density', 'parking_density', 'fuel_density', 'bank_density',\n",
    "            'pub_density', 'telephone_density', 'toilets_density', 'taxi_density', 'bicycle_parking_density',\n",
    "            'motorcycle_parking_density', 'fast_food_density', 'bar_density', 'life_boats_density',\n",
    "            'restaurant_density', 'arts_centre_density', 'music_venue_density', 'nightclub_density',\n",
    "            'cafe_density', 'atm_density', 'community_centre_density', 'jobcentre_density', 'doctors_density',\n",
    "            'cinema_density', 'grave_yard_density', 'police_density', 'bus_station_density', 'theatre_density',\n",
    "            'bureau_de_change_density', 'hospital_density', 'bench_density', 'school_density', 'courthouse_density',\n",
    "            'ice_cream_density', 'fountain_density', 'left_luggage_density', 'drinking_water_density',\n",
    "            'casino_density', 'car_rental_density', 'car_wash_density', 'ferry_terminal_density', 'dentist_density',\n",
    "            'townhall_density', 'shelter_density', 'parking_entrance_density', 'conference_centre_density',\n",
    "            'marketplace_density', 'vending_machine_density', 'waste_basket_density', 'clock_density',\n",
    "            'studio_density', 'veterinary_density', 'gallery_density', 'gambling_density', 'kindergarten_density',\n",
    "            'social_facility_density', 'charging_station_density', 'car_sharing_density', 'clinic_density',\n",
    "            'water_density', 'compressed_air_density', 'public_building_density', 'social_centre_density',\n",
    "            'childcare_density', 'grit_bin_density', 'bicycle_repair_station_density', 'events_venue_density',\n",
    "            'embassy_density', 'college_density', 'circus_school_density', 'parcel_lockers_density',\n",
    "            'money_transfer_density', 'photo_booth_density', 'luggage_locker_density', 'university_density',\n",
    "            'venue_density', 'swimming_pool_density', 'fire_station_density', 'post_depot_density',\n",
    "            'crematorium_density', 'sport_density', 'nursing_home_density', 'biergarten_density', 'garden_density',\n",
    "            'prison_density', 'club_density', 'parking_space_density', 'trailer_park_density', 'archive_density',\n",
    "            'monastery_density']\n",
    "target = ['bike_rental_density']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove features that are very sparse\n",
    "\n",
    "# Get percentage of non-zero values for target variable\n",
    "pcent_target_positive = df[target].astype(bool).sum(axis=0)/len(df)\n",
    "# print(pcent_target_positive)\n",
    "# Get percentage of non zero values for each feature\n",
    "pcent_non_zero_features = df[features].astype(bool).sum(axis=0)/len(df)\n",
    "\n",
    "# print(pcent_non_zero_features)\n",
    "zero_pcent_threshold = 0.01\n",
    "relevant_features = pcent_non_zero_features.index[pcent_non_zero_features>zero_pcent_threshold].tolist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Show correlation between input features\n",
    "corr_df = df[target + relevant_features].corr()\n",
    "\n",
    "f = plt.figure(figsize=(19, 15))\n",
    "plt.matshow(corr_df, fignum=f.number)\n",
    "plt.xticks(range(corr_df.shape[1]), corr_df.columns, fontsize=14, rotation=90)\n",
    "plt.yticks(range(corr_df.shape[1]), corr_df.columns, fontsize=14)\n",
    "cb = plt.colorbar()\n",
    "cb.ax.tick_params(labelsize=14)\n",
    "# plt.title('Correlation Matrix', fontsize=16);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get features that are most correlated to predictions of bike share density\n",
    "corr_threshold = 0.2\n",
    "feature_relevance = (corr_df.bike_rental_density).sort_values(ascending=False)\n",
    "feature_relevance\n",
    "top_features = list(feature_relevance[feature_relevance > corr_threshold].index)\n",
    "# make sure to remove bike_rentals from training data \n",
    "top_features.remove('bike_rental_density')\n",
    "# remove features that are strongly correlated to others\n",
    "top_features.remove('post_box_density') # correlated to car shares\n",
    "top_features.remove('restaurant_density') # correlated to cafes\n",
    "\n",
    "# remove features that are highly correlated to each other, e.g. cafes and restaurants.\n",
    "print(top_features)\n",
    "print('Number of features reduced to %2.0f from %2.0f' % (len(top_features), len(features)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Profile features collected from OSM and check for correlation\n",
    "# feature_profile = pandas_profiling.ProfileReport(df[target + features])\n",
    "# feature_profile.to_file(output_file='osm_bike_share_feature_profile.html')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test\n",
    "X = df[top_features]\n",
    "y = df[target]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "pos_target = len(y[y.bike_rental_density != 0])\n",
    "neg_target = len(y[y.bike_rental_density == 0])\n",
    "print(pos_target,neg_target,pos_target/neg_target)\n",
    "\n",
    "sns.distplot(y.bike_rental_density);\n",
    "print('Linear regression is not the best option given that the target is highly skewed towards 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Linear model\n",
    "LM_model = LinearRegression()\n",
    "LM_model.fit(X_train,y_train)\n",
    "LM_y_pred = LM_model.predict(X_test)\n",
    "\n",
    "## Lasso\n",
    "lasso_rmse = []\n",
    "ridge_rmse = []\n",
    "EN_rmse = []\n",
    "alpha_ref = np.linspace(0.001,1,101)\n",
    "\n",
    "for ind, alpha in enumerate(alpha_ref):\n",
    "    \n",
    "    lasso_model = Lasso(alpha=alpha)\n",
    "    lasso_model.fit(X_train,y_train)\n",
    "    lasso_y_pred = lasso_model.predict(X_test)\n",
    "    lasso_rmse.append(metrics.mean_absolute_error(y_test,lasso_y_pred))\n",
    "    \n",
    "    \n",
    "    ridge_model = Ridge(alpha = alpha)\n",
    "    ridge_model.fit(X_train,y_train)\n",
    "    ridge_y_pred = ridge_model.predict(X_test)\n",
    "    ridge_rmse.append(metrics.mean_absolute_error(y_test,lasso_y_pred))\n",
    "    \n",
    "    EN_model = ElasticNet(alpha = alpha)\n",
    "    EN_model.fit(X_train,y_train)\n",
    "    EN_y_pred = EN_model.predict(X_test)\n",
    "    EN_rmse.append(metrics.mean_absolute_error(y_test,lasso_y_pred))\n",
    "\n",
    "\n",
    "## Random Forest\n",
    "# RF_model = RandomForestRegressor(random_state = 2,max_depth = 9,max_leaf_nodes = 67)\n",
    "RF_model = RandomForestRegressor()\n",
    "RF_model.fit(X_train,y_train)\n",
    "RF_y_pred = RF_model.predict(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Display linear coefficients to see which features are most correlated to bike share density\n",
    "linear_coeffs = pd.DataFrame({'feature_name': top_features, 'Coefficient': LM_model.coef_.flatten()})\n",
    "linear_coeffs.sort_values(by='Coefficient', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Define random grid for tuning random forest parameters\n",
    "\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "pprint(random_grid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use the random grid to search for best RF hyperparameters\n",
    "# Takes a long time, only run if you have time\n",
    "\n",
    "# First create the base model to tune\n",
    "rf2 = RandomForestRegressor()\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "rf_random = RandomizedSearchCV(estimator = rf2, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "# Fit the random search model\n",
    "rf_random.fit(X_train,y_train)\n",
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Predict bike shares using the best RF \n",
    "best_random = rf_random.best_estimator_\n",
    "RF_rand_y_pred = best_random.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save best RF model\n",
    "# save the model to disk\n",
    "filename = 'best_RF_model.sav'\n",
    "joblib.dump(best_random, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation on hold out set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Linear regression isn't the best option given that the target is highly skewed towards 0\n",
    "\n",
    "print('Multivariate linear regression mean absolute error (MAE): %4.3f' % metrics.mean_absolute_error(y_test,LM_y_pred))\n",
    "print('Lasso mean absolute error (MAE): %4.3f and alpha = %4.3f' % (min(lasso_rmse),alpha_ref[np.argmin(lasso_rmse)]))\n",
    "print('Ridge mean absolute error (MAE): %4.3f and alpha = %4.3f' % (min(ridge_rmse),alpha_ref[np.argmin(ridge_rmse)]))\n",
    "print('Elastic Net mean absolute error (MAE): %4.3f and alpha = %4.3f' % (min(EN_rmse),alpha_ref[np.argmin(EN_rmse)]))\n",
    "\n",
    "print('Random forest mean absolute error (MAE): %4.3f' % metrics.mean_absolute_error(y_test,RF_y_pred))\n",
    "print('Best Random forest mean absolute error (MAE): %4.3f' % metrics.mean_absolute_error(y_test,RF_rand_y_pred))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Independent model prediction on Portland\n",
    "\n",
    "Portland is not included in the training set and this is used for independent validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Start validation on unseen city, \n",
    "place = 'Portland, Oregon, USA'\n",
    "\n",
    "# city_grid = gpd.read_file(filename_grid)\n",
    "# all_amenities = gpd.read_file(filename_amenities)\n",
    "# bike_rentals = gpd.read_file(filename_bike_rentals)\n",
    "\n",
    "# Generate city grid for interrogation\n",
    "city = ox.gdf_from_place(place)\n",
    "portland_grid = gpd.read_file(folder_name + 'portland_grid.geojson')\n",
    "portland_ft = gpd.read_file(folder_name + 'portland_feature_target_table.geojson')\n",
    "portland_bike_rentals = gpd.read_file(folder_name + 'portland_bike_rentals.geojson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portland_predict = RF_model.predict(portland_ft[top_features])\n",
    "portland_comparison = portland_grid.copy()\n",
    "portland_comparison['bike_rental_density'] = portland_ft['bike_rental_density'] \n",
    "portland_comparison['RF_prediction'] = portland_predict\n",
    "portland_comparison['bike_rental_diff'] = portland_comparison['RF_prediction'] - portland_comparison['bike_rental_density']\n",
    "# tes\n",
    "portland_comparison\n",
    "\n",
    "scale_factor = max(max(portland_predict),max(portland_comparison.bike_rental_density))\n",
    "\n",
    "portland_comparison['scaled_actual_density'] = portland_ft['bike_rental_density'] / scale_factor\n",
    "portland_comparison['scaled_pred_density'] = portland_comparison['RF_prediction'] / scale_factor\n",
    "\n",
    "print('Random forest mean absolute error (MAE): %4.3f' % metrics.mean_absolute_error(portland_comparison.bike_rental_density,portland_comparison.RF_prediction))\n",
    "print('Random forest mean squared error (MSE): %4.3f' % metrics.mean_squared_error(portland_comparison.bike_rental_density,portland_comparison.RF_prediction))\n",
    "print('Random forest root mean squared error (RMSE): %4.3f' % np.sqrt(metrics.mean_squared_error(portland_comparison.bike_rental_density,portland_comparison.RF_prediction)))\n",
    "\n",
    "\n",
    "# RMSE is low because most of the grid cells are empty and random forest captures that well. Does this misrepresent the accuracty?\n",
    "# Results could be improved with bike friendliness score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define dictionaries for opacity and colormaps\n",
    "pred_dict = portland_comparison['scaled_pred_density']\n",
    "actual_dict = portland_comparison['scaled_actual_density']\n",
    "diff_dict = portland_comparison['bike_rental_diff']\n",
    "\n",
    "pred_opacity = {str(key): pred_dict[key] for key in pred_dict.keys()}\n",
    "actual_opacity = {str(key): actual_dict[key] for key in actual_dict.keys()}\n",
    "diff_opacity = {str(key): abs(diff_dict[key]) for key in diff_dict.keys()}\n",
    "\n",
    "colormap = cm.linear.RdBu_11.scale(-scale_factor,scale_factor)\n",
    "# colormap = cm.LinearColormap(colors=['yellow','white','green'],vmin=0,vmax=scale_factor)\n",
    "\n",
    "diff_color = {str(key): colormap(diff_dict[key]) for key in diff_dict.keys()}\n",
    "colormap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "m = folium.Map([city.geometry.centroid.y, city.geometry.centroid.x],\n",
    "               zoom_start=11,\n",
    "               tiles=\"CartoDb positron\")\n",
    "\n",
    "style_city = {'color':'#ebc923 ', 'fillColor': '#ebc923 ', 'weight':'1', 'fillOpacity' : 0.1}\n",
    "folium.GeoJson(city,\n",
    "               style_function=lambda x: style_city,\n",
    "               name='City Limit').add_to(m)\n",
    "\n",
    "# Plot actual bike share density\n",
    "folium.GeoJson(\n",
    "    portland_comparison['geometry'],\n",
    "    name='Actual bike share density',\n",
    "    style_function=lambda feature: {\n",
    "        'fillColor': '#0675c4',\n",
    "        'color': 'black',\n",
    "        'weight': 0,\n",
    "        'fillOpacity': actual_opacity[feature['id']],\n",
    "    }\n",
    ").add_to(m)\n",
    "\n",
    "# plot predictions of bike share density\n",
    "folium.GeoJson(\n",
    "    portland_comparison['geometry'],\n",
    "    name='Prediction: bike share density',\n",
    "    style_function=lambda feature: {\n",
    "        'fillColor': '#0675c4',\n",
    "        'color': 'black',\n",
    "        'weight': 0,\n",
    "        'fillOpacity': pred_opacity[feature['id']],\n",
    "    }\n",
    ").add_to(m)\n",
    "\n",
    "# plot difference between predicted and actual\n",
    "# folium.GeoJson(\n",
    "#     portland_comparison['geometry'],\n",
    "#     name='Difference: bike share density',\n",
    "#     style_function=lambda feature: {\n",
    "#         'fillColor': '#1FFD09',\n",
    "#         'color': 'black',\n",
    "#         'weight': 0,\n",
    "#         'fillOpacity': diff_opacity[feature['id']],\n",
    "#     }\n",
    "# ).add_to(m)\n",
    "\n",
    "folium.GeoJson(\n",
    "    portland_comparison['geometry'],\n",
    "    name='Difference: bike share density',\n",
    "    style_function=lambda feature: {\n",
    "        'fillColor': diff_color[feature['id']],\n",
    "        'color': 'black',\n",
    "        'weight': 0,\n",
    "#         'fillOpacity': 0.5,\n",
    "        'fillOpacity': diff_opacity[feature['id']],\n",
    "    }\n",
    ").add_to(m)\n",
    "\n",
    "colormap.caption = 'Difference in bike density prediction'\n",
    "colormap.add_to(m)\n",
    "\n",
    "\n",
    "folium.LayerControl().add_to(m)\n",
    "\n",
    "# m.save(\"portland_validation.html\")\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tried to put into categories and compute confusion matrix but prediction is of bike share demand and not categories\n",
    "\n",
    "# positives = portland_comparison[(portland_comparison.bike_rental_density != 0)]\n",
    "# negatives = portland_comparison[(portland_comparison.bike_rental_density == 0)]\n",
    "# pos_count = len(positives)\n",
    "# neg_count = len(negatives)\n",
    "\n",
    "# print('Positives %2.0f, Negatives %2.0f, Total %2.0f' % (pos_count,neg_count,len(portland_comparison)))\n",
    "\n",
    "# density_threshold = 0\n",
    "# true_pos = portland_comparison[(portland_comparison.bike_rental_density != 0) & (portland_comparison.bike_rental_diff > density_threshold)]\n",
    "# false_pos = portland_comparison[(portland_comparison.bike_rental_density == 0) & (portland_comparison.bike_rental_diff > density_threshold)]\n",
    "# true_neg = portland_comparison[(portland_comparison.bike_rental_density == 0) & (portland_comparison.bike_rental_diff < density_threshold)]\n",
    "# false_neg = portland_comparison[(portland_comparison.bike_rental_density != 0) & (portland_comparison.bike_rental_diff < density_threshold)]\n",
    "\n",
    "# TP_count = len(true_pos)\n",
    "# FP_count = len(false_pos)\n",
    "# TN_count = len(true_neg)\n",
    "# FN_count = len(false_neg)\n",
    "\n",
    "# print('True positives %2.0f, False positives %2.0f' % (TP_count,FP_count))\n",
    "# print('True negatives %2.0f, False negative %2.0f' % (TN_count,FN_count))\n",
    "\n",
    "# precision = TP_count/(TP_count+FP_count)\n",
    "# recall = TP_count/pos_count\n",
    "\n",
    "# print('Precision: %4.3f' % precision)\n",
    "# print('Recall: %4.3f' % recall)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
